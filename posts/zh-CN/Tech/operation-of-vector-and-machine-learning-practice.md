---
title: 数学笔记1：向量的运算以及在机器学习中的运用
date: undefined
summary: 本文将介绍向量中的常见运算，以及在机器学习中的推广运用。
---


我们知道向量是既有大小又有方向的量。本文将介绍向量中的常见运算，以及在机器学习中的推广运用。

## 线性运算

对于加法，我们可以用三角形法则或平行四边形法则来计算。

向量的线性运算与常数的线性运算类似，符合如下规律：
- 交换律：a+b=b+a
- 结合律：（a+b)+c=a+(b+c)
- 分配律：（a+b)c=ac + bc

## 点积

点积的运算结果是一个值。

其几何意义是一个向量在另一个向量上的投影长度。

## 叉积

叉积可以表示为：

$$
A × B
$$

**叉积的运算结果是一个新的向量**。具体运算方法如下：

不难发现，叉乘的计算实际上是一个特定的 3x3 行列式的计算，其中第一行是单位向量，后两行是参与叉乘的向量的分量。可以认为向量的叉乘在三维空间中是行列式概念的一个特殊应用。

由此也不难得出叉乘并不满足交换律。

在几何意义上，叉乘的运算结果相当于A和B所构成的平面的法向量。为了方便记忆，你可以使用右手法则：首先要保持拇指朝上，然后其他四指指向叉积的第一个向量，向内弯曲四指指向另一个向量如果两个向量的方向能符合这个手势，此时拇指的方向就是叉积的方向；如果必须向外弯曲四指，拇指的反方向是叉积的方向。

求过 3 个点M_1(2, -1,4), M_2(-1,3,-2),M_3(0,2,3) 的平面方程。

根据点法式的组成可知，我们只需要知道该平面的法向量，就可以写出它的点法式方程。

## 推广1：向量相似度比较

在机器学习中常常遇到比较向量相似度的问题。

两个向量的点积运算结果越大，表示它们越相似。好比倾斜树立的电线杆，其越贴近地面，投影长度越大，就越与地面相似。

## 推广2：矩阵相似度比较

若把矩阵抽象为多行向量，不难想出将其展开为向量的比较方法。然而，这会导致元素内在的联系消失。在真实的学习任务中，例如图像识别，矩阵（像素点）内部之间元素是存在联系的。

常用的方法是使用 SVD （奇异值分解）。这是矩阵的五大分解之一。关于奇异值分解，可以参考笔者的另一篇文章。
